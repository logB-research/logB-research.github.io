<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://logb-research.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://logb-research.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-28T18:48:58+00:00</updated><id>https://logb-research.github.io/feed.xml</id><title type="html">blank</title><subtitle># A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Kernel Trick I - Deep Convolutional Representations in RKHS</title><link href="https://logb-research.github.io/blog/2024/ckn/" rel="alternate" type="text/html" title="Kernel Trick I - Deep Convolutional Representations in RKHS"/><published>2024-04-22T00:00:00+00:00</published><updated>2024-04-22T00:00:00+00:00</updated><id>https://logb-research.github.io/blog/2024/ckn</id><content type="html" xml:base="https://logb-research.github.io/blog/2024/ckn/"><![CDATA[<h2 id="goal-">Goal üöÄ</h2> <blockquote> <p>Fear not, those who delve into the maths of the kernel trick, for its advent in deep learning is coming.</p> </blockquote> <p>In this blog post, we focus on the Convolutional Kernel Network (CKN) architecture introduced in <d-cite key="mairal2016endtoend"></d-cite> and present its guiding principles and main components. This model opened a new line of research by demonstrating the benefits of the kernel trick for deep convolutional representations. For a more complete picture, we invite the reader to refer to the original paper <d-cite key="mairal2016endtoend"></d-cite>.</p> <h2 id="kernel-trick">Kernel Trick</h2> <p>We provide below a simple definition of the kernel trick. For the interested reader, more details about the kernel trick can be found in <d-cite key="scholkopf2000kerneltrick"></d-cite> and the <a href="https://mva-kernel-methods.github.io/course-2023-2024/static_files/materials/slides.pdf">slides</a> of <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a> and <a href="https://jpvert.github.io/">Jean-Philippe Vert</a> contain all one needs to know about kernel methods. Let \(\mathcal{H}\) be a high-dimensional feature space and \(\varphi\colon \mathcal{X} \to \mathcal{H}\) a mapping. In the situation where inputs are first mapped to \(\mathcal{H}\) with \(\varphi\) and then compared using the inner product \(\langle \varphi(x), \varphi(x')\rangle_{\mathcal{H}}\), the kernel trick enables us to directly evaluate such pairwise comparison by kernel evaluation, i.e.,</p> \[\begin{equation*} K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}} \end{equation*}\] <details><summary>:mag_right: More details on Kernels</summary> <p>Let us consider a set \(\mathcal{X}\), a p.d. kernel \(K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}\), its associated RKHS \(\mathcal{H}\) and its associated mapping fonction \(\varphi : \mathcal{X} \to \mathbb R\), i.e. the function such that \(\forall \mathbf{x},\mathbf{x}',~K(x,x') = \langle \varphi(\mathbf{x}),\varphi(\mathbf{x}') \rangle_{\mathcal{H}}\).</p> <p>In the rest of this post, we‚Äôll be talking about CKNs, which use a particular type of kernel, namely dots product kernels. Let us consider some function \(\kappa : \mathbb{R}\to\mathbb{R}\). A dot product kernel is a p.d. kernel \(K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}\) defined as \(\forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \kappa(\langle \mathbf{x},\mathbf{x}'\rangle).\) We also define homogeneous dot product kernels as \(\forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \lVert \mathbf{x} \rVert\lVert \mathbf{x}'\rVert \kappa \left(\left\langle \frac{\mathbf{x}}{\lVert \mathbf{x} \rVert},\frac{\mathbf{x}'}{\lVert \mathbf{x}'\rVert}\right\rangle\right).\)</p> <p>If \(\kappa\) has some regular properties (see <d-cite key="mairal2016endtoend"></d-cite>), we can use homogeneous dot product kernels to represent local image neighborhoods in \(\mathcal{H}\).</p> <p>A lot of these kernels exist, see <d-cite key="mairal2016endtoend"></d-cite> for details. These include the \textit{exponential} dot product kernel \(K_{\exp}(\mathbf{x},\mathbf{x}') = \text{e}^{\beta(\langle \mathbf{x},\mathbf{x}'\rangle -1)} = \text{e}^{-\frac{\beta}{2}\lVert \mathbf{x}-\mathbf{x}' \rVert_2^2}\), where \(\beta &gt; 0\). When \(\displaystyle\beta = \frac{1}{\sigma^2}\), we recover the well-known Gaussian Kernel. By the way, the arguments of the chosen kernels can be learned during model training!</p> </details> <p>A direct and very popular application of the kernel trick is SVM. It allows us to map data points from an initial space to a more abstract RKHS. In this way, we can move into spaces in which the data is represented in a linearly separable way (by a hyperplane). Here‚Äôs a short animation showing how the kernel trick can be used to map data initially contained in a 2D plane, to a larger space where they can be separated by a hyperplane.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/kernel_trick_svm_hq_loop-480.webp 480w,/assets/img/blog_ckn/kernel_trick_svm_hq_loop-800.webp 800w,/assets/img/blog_ckn/kernel_trick_svm_hq_loop-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_ckn/kernel_trick_svm_hq_loop.gif" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Of course, kernel trick can be used for much more than that, and in this post, we propose to show you how you can play with convolutional representations using kernel trick.</p> <h2 id="convolutional-kernel-network-in-depth">Convolutional Kernel Network in depth</h2> <h3 id="mathematical-insights-and-computability">Mathematical insights and computability</h3> <p>Kernel trick here, kernel trick there‚Ä¶ But can we really compute this kernel trick in practice ?</p> <p>Let us define mathematically an image as a mapping \(I_0 : \Omega_0 \to \mathbb{R}^{3}\) (as an image has \(3\) color channels). If \(\mathbf{x}\) and \(\mathbf{x}'\) are two patches extracted from \(I_0\) we can get \(\varphi_1(\mathbf{x})\) and \(\varphi_1(\mathbf{x}')\), their representation in \(\mathcal{H}\), thanks to the kernel trick ! But, \(\mathcal{H}\) is an <strong><em>infinite</em></strong> dimensional space, so how to deal with it in practice? :dizzy_face:</p> <p>No panic, our friend Nystr√∂m is here to help! Nystr√∂m‚Äôs method aims to approximate \(\varphi_1(\mathbf{x})\) and \(\varphi_1(\mathbf{x}')\) by their projection \(\psi_1(\mathbf{x})\) and \(\psi_1(\mathbf{x}')\) onto a <strong><em>finite</em></strong> dimensional subspace \(\mathcal{F}\) (see the figure below).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/Nystrom-480.webp 480w,/assets/img/blog_ckn/Nystrom-800.webp 800w,/assets/img/blog_ckn/Nystrom-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_ckn/Nystrom.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The subspace \(\mathcal{F}\) is defined as \(\mathcal{F} = \text{span}(z_1,\dots,z_p)\), where \((z_i)_{i\in\{1\dots p\}}\) are anchor points, of unit-norm. \(Z = \{z_1,\dots,z_p\}\) are the parameters of the layer in the sense that optimizing the layer means optimizing \(\mathcal{F}\). The subspace \(\mathcal{F}\) can be optimized in both a <em>supervised</em> (with backpropagation rules) or an <em>unsupervised</em> way (by minimzing projection residuals with spherical KMeans), see <d-cite key="mairal2016endtoend"></d-cite>.</p> <h3 id="the-convolutional-kernel-network">The Convolutional Kernel Network</h3> <p>We can therefore construct our Convolutional Kernel Layer in three steps.</p> <ul> <li>We extract patches \(\mathbf{x}\) from the image \(I_0\).</li> </ul> <details><summary>Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">extract_2d_patches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">filter_size</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">unfolded_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">stride_tricks</span><span class="p">.</span><span class="nf">sliding_window_view</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
        <span class="n">unfolded_x</span> <span class="o">=</span> <span class="n">unfolded_x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">patch_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">unfolded_x</span>

<span class="k">def</span> <span class="nf">sample_patches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">n_sampling_patches</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">extract_2d_patches</span><span class="p">(</span><span class="n">x_in</span><span class="p">)</span>
        <span class="n">n_sampling_patches</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">patches</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_sampling_patches</span><span class="p">)</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="n">patches</span><span class="p">[:</span><span class="n">n_sampling_patches</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">patches</span></code></pre></figure> </details> <ul> <li> <table> <tbody> <tr> <td>We normalize and convolve them as $$\lVert \mathbf{x} \rVert \kappa \left( Z^\top \displaystyle\frac{\mathbf{x}}{</td> <td>¬†</td> <td>\mathbf{x}</td> <td>¬†</td> <td>} \right)\(and compute the approximation as\)\psi(x) = \lVert \mathbf{x} \rVert\kappa\left(Z^\top Z\right)^{-1/2}\kappa\left(Z^\top \displaystyle\frac{\mathbf{x}}{</td> <td>¬†</td> <td>\mathbf{x}</td> <td>¬†</td> <td>}\right)\(by applying the linear transform\)\kappa\left(Z^\top Z\right)^{-1/2}$$ at every pixel location,</td> </tr> </tbody> </table> </li> </ul> <details><summary>Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">conv_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x_in</span><span class="p">):</span>
        <span class="n">patch_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nf">conv2d_scipy</span><span class="p">(</span><span class="n">x_in</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">ones</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">padding</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">groups</span><span class="p">),</span><span class="n">a_min</span><span class="o">=</span><span class="n">EPS</span><span class="p">,</span><span class="n">a_max</span><span class="o">=</span><span class="bp">None</span><span class="p">))</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="nf">conv2d_scipy</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dilation</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">x_out</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">patch_norm</span><span class="p">,</span><span class="n">a_min</span><span class="o">=</span><span class="n">EPS</span><span class="p">,</span><span class="n">a_max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">patch_norm</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">kappa</span><span class="p">(</span><span class="n">x_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_out</span>
    
<span class="k">def</span> <span class="nf">mult_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">lintrans</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x_in</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span>
            <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">lintrans</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> 
            <span class="n">x_in</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span></code></pre></figure> </details> <ul> <li>We compute the pooling operations. Note that gaussian linear pooling is defined as</li> </ul> \[\begin{equation*} \displaystyle I_1(x) = \int_{\mathbf{x}'\in\Omega_0} M_1(x') \text{e}^{-\beta\lVert \mathbf{x}-\mathbf{x}'\rVert_2^2}\text{d}\mathbf{x}' \end{equation*}\] <p>where \(M_1\) is the ‚Äúfeature map‚Äù after the second point operation. That is why, we can interpret the pooling operation as a ‚Äúconvolution‚Äù operation.</p> <details><summary>Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">pool_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x_in</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">subsampling</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_in</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="nf">conv2d_scipy</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">pooling_filter</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
            <span class="n">stride</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">subsampling</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">subsampling</span><span class="p">,</span> 
            <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_out</span></code></pre></figure> </details> <p>Here is a global figure that summarizes all these operations, and thus the construction of a Convolutional Kernel Layer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/CKN-480.webp 480w,/assets/img/blog_ckn/CKN-800.webp 800w,/assets/img/blog_ckn/CKN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_ckn/CKN.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>After these operations, we have constructed a ‚Äúfeature map‚Äù \(I_1 : \Omega_1 \to \mathbb{R}^{p_1}\), that can be re-used. We can now build a <strong>Convolutional Kernel Network</strong> by stacking Convolutional Kernel Layers, and we will have a <em>representation</em> of the image at the output.</p> <h2 id="cnn-vs-ckn">CNN vs. CKN</h2> <p>In <d-cite key="bietti2018group"></d-cite>, it is shown that CKNs contain a large class of CNNs with smooth homogeneous activation functions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/CNN-480.webp 480w,/assets/img/blog_ckn/CNN-800.webp 800w,/assets/img/blog_ckn/CNN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_ckn/CNN.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The similarities and differences between CKN and CNN are well illustrated in Figures~\ref{fig:CKN2} and \ref{fig:CNN}.</p> <p>On the one hand, A CNN of \(K\) layer can be represented by its output \(f_{\text{CNN}}(\mathbf{x})\), if \(\mathbf{x}\) is the input, as :</p> \[\begin{equation*} f_{\text{CNN}}(\mathbf{x}) = \gamma_K(\sigma_K(W_K\dots \gamma_2(\sigma_2(W_2\gamma_1(\sigma_1(W_1\mathbf{x}))\dots)) \end{equation*}\] <p>where \((W_k)_k\) represent the convolution operations, \((\sigma_k)_k\) are pointwise non-linear functions, (e.g., ReLU), and \((\gamma_k)_k\) represent the pooling operations (see <d-cite key="paulin2016convolutional"></d-cite>).</p> <p>On the other hand, A CKN of \(K\) layer can be represented by its output \(f_{\text{CKN}}(\mathbf{x})\), if \(\mathbf{x}\) is the input, as :</p> \[\begin{equation*} f_{\text{CKN}}(\mathbf{x}) = \gamma_K(\sigma_K(W_K(P_K\dots \gamma_2(\sigma_2(W_2(P_2(\gamma_1(\sigma_1(W_1(P_1(\mathbf{x}))\dots)) \end{equation*}\] <p>where \((P_k)_k\) represent the patch extractions, \((W_k)_k\) the convolution operations, \((\sigma_k)_k\) the kernel operations (which allows us to learn non-linearity in the RKHS), and \((\gamma_k)_k\) the pooling operations.</p> <h2 id="desktop_computer-for-the-interesting-reader">:desktop_computer: For the interesting reader</h2> <p>We provide an open-source implementation of the CKN architecture in pure numpy <a href="https://github.com/ozekri/CKN_from_Scratch">here</a> to better understand how things work without having to read pages of documentation of modern deep learning framework such as <code class="language-plaintext highlighter-rouge">PyTorch</code>, <code class="language-plaintext highlighter-rouge">TensorFlow</code> or <code class="language-plaintext highlighter-rouge">JAX</code>. For practical usage, the original implementation can be found <a href="https://github.com/claying/CKN-Pytorch-image">here</a>.</p> <p>For any further questions, please feel free to contact the authors by mail!</p> <h2 id="acknowledgments">Acknowledgments</h2> ]]></content><author><name>Oussama Zekri</name></author><category term="kernel trick"/><category term="convolutional networks"/><category term="deep learning"/><category term="maths"/><category term="code"/><summary type="html"><![CDATA[Convolutional Kernel Networks]]></summary></entry></feed>