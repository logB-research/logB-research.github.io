<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Kernel Trick I - Deep Convolutional Representations in RKHS | logB </title> <meta name="author" content="logB "> <meta name="description" content="Convolutional Kernel Networks"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://logb-research.github.io/blog/2024/ckn/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Kernel Trick I - Deep Convolutional Representations in RKHS",
            "description": "Convolutional Kernel Networks",
            "published": "April 22, 2024",
            "authors": [
              
              {
                "author": "Oussama Zekri",
                "authorURL": "https://oussamazekri.fr",
                "affiliations": [
                  {
                    "name": "ENS Paris-Saclay",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ambroise Odonnat",
                "authorURL": "https://ambroiseodt.github.io/",
                "affiliations": [
                  {
                    "name": "Huawei Noah's Ark Lab",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">logB</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Kernel Trick I - Deep Convolutional Representations in RKHS</h1> <p>Convolutional Kernel Networks</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#equations">Equations</a> </div> <div> <a href="#citations">Citations</a> </div> <div> <a href="#footnotes">Footnotes</a> </div> <div> <a href="#code-blocks">Code Blocks</a> </div> <div> <a href="#interactive-plots">Interactive Plots</a> </div> <div> <a href="#layouts">Layouts</a> </div> <div> <a href="#other-typography">Other Typography?</a> </div> </nav> </d-contents> <h2 id="goal-">Goal üöÄ</h2> <blockquote> <p>Fear not, those who delve into the maths of the kernel trick, for its advent in deep learning is coming.</p> </blockquote> <p>In this blog post, we focus on the Convolutional Kernel Network (CKN) architecture introduced in <d-cite key="mairal2016endtoend"></d-cite> and present its guiding principles and main components. This model opened a new line of research by demonstrating the benefits of the kernel trick for deep convolutional representations. For a more complete picture, we invite the reader to refer to the original paper <d-cite key="mairal2016endtoend"></d-cite>.</p> <h2 id="kernel-trick">Kernel Trick</h2> <p>We provide below a simple definition of the kernel trick. For the interested reader, more details about the kernel trick can be found in <d-cite key="scholkopf2000kerneltrick"></d-cite> and the <a href="https://mva-kernel-methods.github.io/course-2023-2024/static_files/materials/slides.pdf" rel="external nofollow noopener" target="_blank">slides</a> of <a href="https://lear.inrialpes.fr/people/mairal/" rel="external nofollow noopener" target="_blank">Julien Mairal</a> and <a href="https://jpvert.github.io/" rel="external nofollow noopener" target="_blank">Jean-Philippe Vert</a> contain all one needs to know about kernel methods. Let \(\mathcal{H}\) be a high-dimensional feature space and \(\varphi\colon \mathcal{X} \to \mathcal{H}\) a mapping. In the situation where inputs are first mapped to \(\mathcal{H}\) with \(\varphi\) and then compared using the inner product \(\langle \varphi(x), \varphi(x')\rangle_{\mathcal{H}}\), the kernel trick enables us to directly evaluate such pairwise comparison by kernel evaluation, i.e.,</p> \[\begin{equation*} K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}} \end{equation*}\] <details><summary><img class="emoji" title=":mag_right:" alt=":mag_right:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png" height="20" width="20"> More details on Kernels</summary> <p>Let us consider a set \(\mathcal{X}\), a p.d. kernel \(K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}\), its associated RKHS \(\mathcal{H}\) and its associated mapping fonction \(\varphi : \mathcal{X} \to \mathbb R\), i.e. the function such that \(\forall \mathbf{x},\mathbf{x}',~K(x,x') = \langle \varphi(\mathbf{x}),\varphi(\mathbf{x}') \rangle_{\mathcal{H}}\).</p> <p>In the rest of this post, we‚Äôll be talking about CKNs, which use a particular type of kernel, namely dots product kernels. Let us consider some function \(\kappa : \mathbb{R}\to\mathbb{R}\). A dot product kernel is a p.d. kernel \(K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}\) defined as \(\forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \kappa(\langle \mathbf{x},\mathbf{x}'\rangle).\) We also define homogeneous dot product kernels as \(\forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \lVert \mathbf{x} \rVert\lVert \mathbf{x}'\rVert \kappa \left(\left\langle \frac{\mathbf{x}}{\lVert \mathbf{x} \rVert},\frac{\mathbf{x}'}{\lVert \mathbf{x}'\rVert}\right\rangle\right).\)</p> <p>If \(\kappa\) has some regular properties (see <d-cite key="mairal2016endtoend"></d-cite>), we can use homogeneous dot product kernels to represent local image neighborhoods in \(\mathcal{H}\).</p> <p>A lot of these kernels exist, see <d-cite key="mairal2016endtoend"></d-cite> for details. These include the \textit{exponential} dot product kernel \(K_{\exp}(\mathbf{x},\mathbf{x}') = \text{e}^{\beta(\langle \mathbf{x},\mathbf{x}'\rangle -1)} = \text{e}^{-\frac{\beta}{2}\lVert \mathbf{x}-\mathbf{x}' \rVert_2^2}\), where \(\beta &gt; 0\). When \(\displaystyle\beta = \frac{1}{\sigma^2}\), we recover the well-known Gaussian Kernel. By the way, the arguments of the chosen kernels can be learned during model training!</p> </details> <div class="l-page"> <iframe src="/assets/plotly/kernel_trick.html" frameborder="0" scrolling="no" height="200px" width="500px" style="border: 1px dashed grey;"></iframe> </div> <h2 id="convolutional-kernel-network-in-depth">Convolutional Kernel Network in depth</h2> <h3 id="mathematical-insights-and-computability">Mathematical insights and computability</h3> <p>Kernel trick here, kernel trick there‚Ä¶ But can we really compute this kernel trick in practice ?</p> <p>Let us define mathematically an image as a mapping \(I_0 : \Omega_0 \to \mathbb{R}^{3}\) (as an image has \(3\) color channels). If \(\mathbf{x}\) and \(\mathbf{x}'\) are two patches extracted from \(I_0\) we can get \(\varphi_1(\mathbf{x})\) and \(\varphi_1(\mathbf{x}')\), their representation in \(\mathcal{H}\), thanks to the kernel trick ! But, \(\mathcal{H}\) is an <strong><em>infinite</em></strong> dimensional space, so how to deal with it in practice? <img class="emoji" title=":dizzy_face:" alt=":dizzy_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f635.png" height="20" width="20"></p> <p>No panic, our friend Nystr√∂m is here to help! Nystr√∂m‚Äôs method aims to approximate \(\varphi_1(\mathbf{x})\) and \(\varphi_1(\mathbf{x}')\) by their projection \(\psi_1(\mathbf{x})\) and \(\psi_1(\mathbf{x}')\) onto a <strong><em>finite</em></strong> dimensional subspace \(\mathcal{F}\) (see the figure below).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/Nystrom-480.webp 480w,/assets/img/blog_ckn/Nystrom-800.webp 800w,/assets/img/blog_ckn/Nystrom-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog_ckn/Nystrom.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The subspace \(\mathcal{F}\) is defined as \(\mathcal{F} = \text{span}(z_1,\dots,z_p)\), where \((z_i)_{i\in\{1\dots p\}}\) are anchor points, of unit-norm. \(Z = \{z_1,\dots,z_p\}\) are the parameters of the layer in the sense that optimizing the layer means optimizing \(\mathcal{F}\). The subspace \(\mathcal{F}\) can be optimized in both a <em>supervised</em> (with backpropagation rules) or an <em>unsupervised</em> way (by minimzing projection residuals with spherical KMeans), see <d-cite key="mairal2016endtoend"></d-cite>.</p> <h3 id="the-convolutional-kernel-network">The Convolutional Kernel Network</h3> <p>We can therefore construct our Convolutional Kernel Layer in three steps.</p> <ul> <li>We extract patches \(\mathbf{x}\) from the image \(I_0\).</li> </ul> <details><summary>Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">extract_2d_patches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">filter_size</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">unfolded_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">stride_tricks</span><span class="p">.</span><span class="nf">sliding_window_view</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
        <span class="n">unfolded_x</span> <span class="o">=</span> <span class="n">unfolded_x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">patch_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">unfolded_x</span>

<span class="k">def</span> <span class="nf">sample_patches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">n_sampling_patches</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">extract_2d_patches</span><span class="p">(</span><span class="n">x_in</span><span class="p">)</span>
        <span class="n">n_sampling_patches</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">patches</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_sampling_patches</span><span class="p">)</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="n">patches</span><span class="p">[:</span><span class="n">n_sampling_patches</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">patches</span></code></pre></figure> </details> <ul> <li> <table> <tbody> <tr> <td>We normalize and convolve them as $$\lVert \mathbf{x} \rVert \kappa \left( Z^\top \displaystyle\frac{\mathbf{x}}{</td> <td>¬†</td> <td>\mathbf{x}</td> <td>¬†</td> <td>} \right)\(and compute the approximation as\)\psi(x) = \lVert \mathbf{x} \rVert\kappa\left(Z^\top Z\right)^{-1/2}\kappa\left(Z^\top \displaystyle\frac{\mathbf{x}}{</td> <td>¬†</td> <td>\mathbf{x}</td> <td>¬†</td> <td>}\right)\(by applying the linear transform\)\kappa\left(Z^\top Z\right)^{-1/2}$$ at every pixel location,</td> </tr> </tbody> </table> </li> </ul> <details><summary>Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">conv_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x_in</span><span class="p">):</span>
        <span class="n">patch_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nf">conv2d_scipy</span><span class="p">(</span><span class="n">x_in</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">ones</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">padding</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">groups</span><span class="p">),</span><span class="n">a_min</span><span class="o">=</span><span class="n">EPS</span><span class="p">,</span><span class="n">a_max</span><span class="o">=</span><span class="bp">None</span><span class="p">))</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="nf">conv2d_scipy</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dilation</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">x_out</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">patch_norm</span><span class="p">,</span><span class="n">a_min</span><span class="o">=</span><span class="n">EPS</span><span class="p">,</span><span class="n">a_max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">patch_norm</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">kappa</span><span class="p">(</span><span class="n">x_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_out</span>
    
<span class="k">def</span> <span class="nf">mult_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">lintrans</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x_in</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span>
            <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">lintrans</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> 
            <span class="n">x_in</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span></code></pre></figure> </details> <ul> <li>We compute the pooling operations. Note that gaussian linear pooling is defined as</li> </ul> \[\begin{equation*} \displaystyle I_1(x) = \int_{\mathbf{x}'\in\Omega_0} M_1(x') \text{e}^{-\beta\lVert \mathbf{x}-\mathbf{x}'\rVert_2^2}\text{d}\mathbf{x}' \end{equation*}\] <p>where \(M_1\) is the ‚Äúfeature map‚Äù after the second point operation. That is why, we can interpret the pooling operation as a ‚Äúconvolution‚Äù operation.</p> <details><summary>Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">pool_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x_in</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">subsampling</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_in</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="nf">conv2d_scipy</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">pooling_filter</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
            <span class="n">stride</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">subsampling</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">subsampling</span><span class="p">,</span> 
            <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_out</span></code></pre></figure> </details> <p>Here is a global figure that summarizes all these operations, and thus the construction of a Convolutional Kernel Layer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/CKN-480.webp 480w,/assets/img/blog_ckn/CKN-800.webp 800w,/assets/img/blog_ckn/CKN-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog_ckn/CKN.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>After these operations, we have constructed a ‚Äúfeature map‚Äù \(I_1 : \Omega_1 \to \mathbb{R}^{p_1}\), that can be re-used. We can now build a <strong>Convolutional Kernel Network</strong> by stacking Convolutional Kernel Layers, and we will have a <em>representation</em> of the image at the output.</p> <h2 id="cnn-vs-ckn">CNN vs. CKN</h2> <p>In <d-cite key="bietti2018group"></d-cite>, it is shown that CKNs contain a large class of CNNs with smooth homogeneous activation functions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/CNN-480.webp 480w,/assets/img/blog_ckn/CNN-800.webp 800w,/assets/img/blog_ckn/CNN-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog_ckn/CNN.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The similarities and differences between CKN and CNN are well illustrated in Figures~\ref{fig:CKN2} and \ref{fig:CNN}.</p> <p>On the one hand, A CNN of \(K\) layer can be represented by its output \(f_{\text{CNN}}(\mathbf{x})\), if \(\mathbf{x}\) is the input, as :</p> \[\begin{equation*} f_{\text{CNN}}(\mathbf{x}) = \gamma_K(\sigma_K(W_K\dots \gamma_2(\sigma_2(W_2\gamma_1(\sigma_1(W_1\mathbf{x}))\dots)) \end{equation*}\] <p>where \((W_k)_k\) represent the convolution operations, \((\sigma_k)_k\) are pointwise non-linear functions, (e.g., ReLU), and \((\gamma_k)_k\) represent the pooling operations (see <d-cite key="paulin2016convolutional"></d-cite>).</p> <p>On the other hand, A CKN of \(K\) layer can be represented by its output \(f_{\text{CKN}}(\mathbf{x})\), if \(\mathbf{x}\) is the input, as :</p> \[\begin{equation*} f_{\text{CKN}}(\mathbf{x}) = \gamma_K(\sigma_K(W_K(P_K\dots \gamma_2(\sigma_2(W_2(P_2(\gamma_1(\sigma_1(W_1(P_1(\mathbf{x}))\dots)) \end{equation*}\] <p>where \((P_k)_k\) represent the patch extractions, \((W_k)_k\) the convolution operations, \((\sigma_k)_k\) the kernel operations (which allows us to learn non-linearity in the RKHS), and \((\gamma_k)_k\) the pooling operations.</p> <h2 id="desktop_computer-for-the-interesting-reader"> <img class="emoji" title=":desktop_computer:" alt=":desktop_computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5a5.png" height="20" width="20"> For the interesting reader</h2> <p>We provide an open-source implementation of the CKN architecture in pure numpy <a href="https://github.com/ozekri/CKN_from_Scratch" rel="external nofollow noopener" target="_blank">here</a> to better understand how things work without having to read pages of documentation of modern deep learning framework such as <code class="language-plaintext highlighter-rouge">PyTorch</code>, <code class="language-plaintext highlighter-rouge">TensorFlow</code> or <code class="language-plaintext highlighter-rouge">JAX</code>. For practical usage, the original implementation can be found <a href="https://github.com/claying/CKN-Pytorch-image" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>For any further questions, please feel free to contact the authors by mail!</p> <h2 id="acknowledgments">Acknowledgments</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-04-22-ckn.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"logB-research/logB-research.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 logB . </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>