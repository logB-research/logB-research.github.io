---
layout: distill
title: Kernel Trick I
description: Deep Convolutional Representations in RKHS
tags: [kernel trick, convolutional networks, deep learning]
giscus_comments: true
date: 2024-04-22
featured: false

authors:
  - name: Oussama Zekri
    url: "https://oussamazekri.fr"
    affiliations:
      name: ENS Paris-Saclay
  - name: Ambroise Odonnat
    url: "https://ambroiseodt.github.io/"
    affiliations:
      name: Huawei Noah's Ark Lab

bibliography: 2024-04-22-ckn.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Interactive Plots
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Goal üöÄ
> Fear not, those who delve into the maths of the kernel trick, for its advent in deep learning is coming.

In this blog post, we focus on the Convolutional Kernel Network (CKN) architecture introduced in <d-cite key="mairal2016endtoend"></d-cite> and present its guiding principles and main components. This model opened a new line of research by demonstrating the benefits of the kernel trick for deep convolutional representations. For a more complete picture, we invite the reader to refer to the original paper <d-cite key="mairal2016endtoend"></d-cite>. 

## Kernel Trick
We provide below a simple definition of the kernel trick. For the interested reader, more details about the kernel trick can be found in <d-cite key="scholkopf2000kerneltrick"></d-cite> and the [slides](https://mva-kernel-methods.github.io/course-2023-2024/static_files/materials/slides.pdf) of [Julien Mairal](https://lear.inrialpes.fr/people/mairal/) and [Jean-Philippe Vert](https://jpvert.github.io/) contain all one needs to know about kernel methods. Let $$\mathcal{H}$$ be a high-dimensional feature space and $$\varphi\colon \mathcal{X} \to \mathcal{H}$$ a mapping. In the situation where inputs are first mapped to $$\mathcal{H}$$ with $$\varphi$$ and then compared using the inner product $$\langle \varphi(x), \varphi(x')\rangle_{\mathcal{H}}$$, the kernel trick enables us to directly evaluate such pairwise comparison by kernel evaluation, i.e.,

$$
\begin{equation*}
K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}}
\end{equation*}
$$

{% details :mag_right: More details on Kernels %}
Let us consider a set $$\mathcal{X}$$, a p.d. kernel $$K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}$$, its associated RKHS $$\mathcal{H}$$ and its associated mapping fonction $$\varphi : \mathcal{X} \to \mathbb R$$, i.e. the function such that $$\forall \mathbf{x},\mathbf{x}',~K(x,x') = \langle \varphi(\mathbf{x}),\varphi(\mathbf{x}') \rangle_{\mathcal{H}}$$.

We recall below the notions of homogeneous dot product kernels that will be useful in the following. Let us consider some function $\kappa : \mathbb{R}\to\mathbb{R}$. A dot product kernel is a p.d. kernel $K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}$ defined as $$\forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \kappa(\langle \mathbf{x},\mathbf{x}'\rangle).$$
    We also define homogeneous dot product kernels as $$\forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \lVert \mathbf{x} \rVert\lVert \mathbf{x}'\rVert \kappa \left(\left\langle \frac{\mathbf{x}}{\lVert \mathbf{x} \rVert},\frac{\mathbf{x}'}{\lVert \mathbf{x}'\rVert}\right\rangle\right).$$

If $$\kappa$$ has some regular properties (see <d-cite key="mairal2016endtoend"></d-cite>), we can use homogeneous dot product kernels to represent local image neighborhoods in $$\mathcal{H}$$.

A lot of these kernels exist, see <d-cite key="mairal2016endtoend"></d-cite> for details. These include the \textit{exponential} dot product kernel $$K_{\exp}(\mathbf{x},\mathbf{x}') = \text{e}^{\beta(\langle \mathbf{x},\mathbf{x}'\rangle -1)} = \text{e}^{-\frac{\beta}{2}\lVert \mathbf{x}-\mathbf{x}' \rVert_2^2}$$, where $$\beta > 0$$. When $$\displaystyle\beta = \frac{1}{\sigma^2}$$, we recover the well-known Gaussian Kernel. By the way, the arguments of the chosen kernels can be learned during model training!
{% enddetails %}

## 1. Convolutional Kernel Network in depth


### 1.1. Mathematical insights and computability
Kernel trick here, kernel trick there... But can we really compute this kernel trick in practice ?

Let us define mathematically an image as a mapping $$I_0 : \Omega_0 \to \mathbb{R}^{3}$$ (as an image has $3$ color channels). If $$\mathbf{x}$$ and $$\mathbf{x}'$$ are two patches extracted from $$I_0$$ we can get $$\varphi_1(\mathbf{x})$$ and $$\varphi_1(\mathbf{x}')$$, their representation in $$\mathcal{H}$$, thanks to the kernel trick ! But, $$\mathcal{H}$$ is an ***infinite*** dimensional space, so how to deal with it in practice? :dizzy_face:

No panic, our friend Nystr√∂m is here to help! Nystr√∂m's method aims to approximate $$\varphi_1(\mathbf{x})$$ and $$\varphi_1(\mathbf{x}')$$ by their projection $$\psi_1(\mathbf{x})$$ and $$\psi_1(\mathbf{x}')$$ onto a ***finite*** dimensional subspace $$\mathcal{F}$$ (see the figure below).

{% include figure.liquid path="assets/img/blog_ckn/Nystrom.png" class="img-fluid rounded z-depth-0" zoomable=true %}

The subspace $$\mathcal{F}$$ is defined as $$\mathcal{F} = \text{span}(z_1,\dots,z_p)$$, where $$(z_i)_{i\in\{1\dots p\}}$$ are anchor points, of unit-norm. $$Z = \{z_1,\dots,z_p\}$$ are the parameters of the layer in the sense that optimizing the layer means optimizing $$\mathcal{F}$$. The subspace $$\mathcal{F}$$ can be optimized in both a *supervised* (with backpropagation rules) or an *unsupervised* way (by minimzing projection residuals with spherical KMeans), see <d-cite key="mairal2016endtoend"></d-cite>.


### 3. The Convolutional Kernel Network


We can therefore construct our Convolutional Kernel Layer in three steps.

1. The idea is that we first extract patches $$\mathbf{x}$$ from the image $$I_0$$.

{% details Code %}
{% highlight python %}
def extract_2d_patches(self,x):
        h,w = self.filter_size
        batch_size, C, _, _ = x.shape
        unfolded_x = np.lib.stride_tricks.sliding_window_view(x, (batch_size, C, h, w))
        unfolded_x = unfolded_x.reshape(-1, self.patch_dim)
        return unfolded_x

def sample_patches(self, x_in, n_sampling_patches=1000):
        patches = self.extract_2d_patches(x_in)
        n_sampling_patches = min(patches.shape[0], n_sampling_patches)
        patches = patches[:n_sampling_patches]
        return patches
{% endhighlight %}
{% enddetails %}

2. Then, we normalize and convolve them as $$\lVert \mathbf{x} \rVert\kappa\left(Z^\top \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||}\right)$$ and compute the approximation as $$\psi(x) = \lVert \mathbf{x} \rVert\kappa\left(Z^\top Z\right)^{-1/2}\kappa\left(Z^\top \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||}\right)$$ by applying the linear transform $\kappa\left(Z^\top Z\right)^{-1/2}$ at every pixel location,

{% details Code %}
{% highlight python %}
def conv_layer(self,x_in):
        patch_norm = np.sqrt(np.clip(conv2d_scipy(x_in**2,self.ones,bias=None,
                stride=1,padding=self.padding,dilation=self.dilation,
                groups=self.groups),a_min=EPS,a_max=None))
        x_out = conv2d_scipy(x_in, self.weight, self.bias,(1,1),
                        self.padding, self.dilation, self.groups)
        x_out = x_out / np.clip(patch_norm,a_min=EPS,a_max=None)
        x_out = patch_norm * self.kappa(x_out)
        return x_out
    
def mult_layer(self, x_in, lintrans):
        batch_size, in_c, H, W = x_in.shape
        x_out = np.matmul(
            np.tile(lintrans, (batch_size, 1, 1)), 
            x_in.reshape(batch_size, in_c, -1))
        return x_out.reshape(batch_size, in_c, H, W)
{% endhighlight %}
{% enddetails %}

3. We can therefore compute the pooling operations. Note that gaussian linear pooling is defined as
\begin{equation*}
\displaystyle I_1(x) = \int_{\mathbf{x}'\in\Omega_0} M_1(x') \text{e}^{-\beta\lVert \mathbf{x}-\mathbf{x}'\rVert_2^2}\text{d}\mathbf{x}'
\end{equation*}
$$
where $$M_1$$ is the "feature map" after the operation of step 2. That is why, we can interpret the pooling operation as a "convolution" operation.

{% details Code %}
{% highlight python %}
def pool_layer(self,x_in):
        if self.subsampling <= 1:
            return x_in
        x_out = conv2d_scipy(x_in, self.pooling_filter, bias=None, 
            stride=self.subsampling, padding=self.subsampling, 
            groups=self.out_channels)
        return x_out
{% endhighlight %}
{% enddetails %}

Here's a global figure that summarizes all these operations, and thus the construction of a Convolutional Kernel Layer.

{% include figure.liquid path="assets/img/blog_ckn/CKN.pdf" class="img-fluid rounded z-depth-0" zoomable=true %}

After these operations, we have constructed a "feature map" $$I_1 : \Omega_1 \to \mathbb{R}^{p_1}$$, that can be re-used. We can now build a **Convolutional Kernel Network** by stacking Convolutional Kernel Layers, and we will have a *representation* of the image at the output.

## 2. CNN vs. CKN

In <d-cite key="bietti2018group"></d-cite>, it is shown that CKNs contain a large class of CNNs with smooth homogeneous activation functions.

{% include figure.liquid path="assets/img/blog_ckn/CNN.pdf" class="img-fluid rounded z-depth-0" zoomable=true %}

The similarities and differences between CKN and CNN are well illustrated in Figures~\ref{fig:CKN2} and \ref{fig:CNN}.

On the one hand, A CNN of $K$ layer can be represented by its output $$f_{\text{CNN}}(\mathbf{x})$$, if $$\mathbf{x}$$ is the input, as :

$$
\begin{equation*}
f_{\text{CNN}}(\mathbf{x}) = \gamma_K(\sigma_K(W_K\dots \gamma_2(\sigma_2(W_2\gamma_1(\sigma_1(W_1\mathbf{x}))\dots))
\end{equation*}
$$

where $$(W_k)_k$$ represent the convolution operations, $$(\sigma_k)_k$$ are pointwise non-linear functions, (e.g., ReLU), and $$(\gamma_k)_k$$ represent the pooling operations (see <d-cite key="paulin2016convolutional"></d-cite>).

On the other hand, A CKN of $$K$$ layer can be represented by its output $$f_{\text{CKN}}(\mathbf{x})$$, if $$\mathbf{x}$$ is the input, as :

$$
\begin{equation*}
f_{\text{CKN}}(\mathbf{x}) = \gamma_K(\sigma_K(W_K(P_K\dots \gamma_2(\sigma_2(W_2(P_2(\gamma_1(\sigma_1(W_1(P_1(\mathbf{x}))\dots))
\end{equation*}
$$

where $$(P_k)_k$$ represent the patch extractions, $$(W_k)_k$$ the convolution operations, $$(\sigma_k)_k$$ the kernel operations (which allows us to learn non-linearity in the RKHS), and $$(\gamma_k)_k$$ the pooling operations.

## :desktop_computer: For the interesting reader

We provide an open-source implementation of the CKN architecture in pure numpy [here](https://github.com/ozekri/CKN_from_Scratch) to better understand how things work without having to read pages of documentation of modern deep learning framework such as `PyTorch`, `TensorFlow` or `JAX`. For practical usage, the original implementation can be found [here](https://github.com/claying/CKN-Pytorch-image). 

For any further questions, please feel free to contact the authors by mail!

## Acknowledgments

We heartfully thank the professors of the [Kernel Methods course](https://mva-kernel-methods.github.io/course-2023-2024/) of the [MVA Master](https://www.master-mva.com/) : Prof. [Julien Mairal](https://lear.inrialpes.fr/people/mairal/), Prof. [Jean-Philippe Vert](https://jpvert.github.io/), Prof. [Michel Arbel](https://michaelarbel.github.io/), Prof. [Alessandro Rudi](https://www.di.ens.fr/~rudi/). A special mention to Prof. [Julien Mairal](https://lear.inrialpes.fr/people/mairal/) for proofreading this blog post !
